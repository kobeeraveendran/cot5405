\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[legalpaper, portrait, margin=0.8in]{geometry}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{hyperref}

\title{Homework 2 -- COT5405
    \thanks{For some questions, I received guidance from or collaborated with: TA (office hours)}
}
\author{Kobee Raveendran}
\date{September 2021}

\begin{document}

\maketitle

\begin{enumerate}
    \item \textbf{Prove or disprove that a HEAP can support the following operations in $o(log n)$ time: 
    (i) insert($x$): insert an element $x$ into the HEAP and (ii) \texttt{extract\_min\_max}: extract either 
    the minimum or the maximum element (the HEAP won't tell if the extracted element is the min 
    or max) [Hint: Prove by contraction via sorting lower bound].}
    \item \textbf{A binomial heap can perform both insertions and extract min operations in $O(\log n)$ 
    worst-case time. By proposing an appropriate potential function, show that the amortized 
    cost of insertion is $O(1)$ and the amortized cost of extract min is $O(\log n)$.}
    \item \textbf{[Range Query Data Structure]: Let S be a (dynamic) set of items, where each item is 
    associated with a unique ``key'' (a positive number) and a unique ``weight'' (a positive 
    number). Therefore each item can be represented as a (key, weight) pair. Design an $O(n)$ 
    space data structure that can support the following operations on S in O(log n) time, where 
    n is the cardinality of S: (i) insert a new item into S (ii) RangeQuery[a,b]: report the 
    item with the highest weight among all items in S with the key within the range [a,b].}

    We can approach this problem using a modified form of a binary search tree. To see how, I'll first be laying out 
    some things we need to store in each node of our data structure for $S$:

    \begin{itemize}
        \item the key for a node, which will be the primary means of ``indexing'' in our data structure
        \item the weight/value for the node
        \item the maximum weight of the node's left subtree
        \item the maximum weight of the node's right subtree
    \end{itemize}

    The reason I choose to store the max weights of both subtrees is to make searching for the max weight 
    take less than an additional $O(k)$ time, where $k = b - a$. If these values were not stored, we would first 
    have to find the smallest key $\geq a$ and the largest key $\leq b$, and then search every node with keys 
    between those values exhaustively. But, since we maintain and update these values as we insert nodes into the 
    tree, all we have to do is find the node $\geq a$ and the node $\leq b$, and do a series of $O(1)$ comparison between 
    a couple of values along the way between the two. The comparisons needed would follow these cases:

    Putting this all together, here's a more exact description of how the data structure and its functions:

    \begin{itemize}
        \item inserting a new item into the data structure:
        \begin{enumerate}
            \item create a node that contains the given key, the given value, and \texttt{maxleft} and \texttt{maxright}, which 
            may be initialized to $-\infty$
            \item insert the node into the data structure following a BST's insertion steps; that is, do a binary search 
            for the node's key in the tree and create a left/right child for the last node found, as appropriate
            \item as you are traversing the array in this binary search fashion, also update the ``maxleft'' or ``maxright'' values 
            of the nodes you pass through as needed. For example, if insertion makes the node to be inserted go right of the current node, 
            update the current node's ``maxright'' value with $max(\texttt{curr.right}, \texttt{node.weight})$. This 
            ensures that the ``left'' and ``right'' values of all nodes are kept up-to-date as we insert more nodes. 
            Since this comparison incurs only constant time for each step spent traversing the tree, we can keep this in 
            $O(\log n)$.
        \end{enumerate}

        \item RangeQuery[a, b]:
        \begin{enumerate}
            \item find the node with key $\geq a$ using binary search. Store the current max value seen, which will just 
            be \texttt{curr.weight}
            \item we now search for the node with key $\leq b$ (which may or may not be in the subtree rooted at the 
            node found in (a)). To account for cases where a node above the node with key $\leq b$ has a higher weight, 
            we must walk along the path between $a$ and $b$ and check the max values as we go along.
            \item as we move along in search of the node with key $\leq b$, we compare our current max value with 
            the max of the current node's weight and its \texttt{maxleft} value, and take it if it's larger. By doing this, 
            means we don't have to actually search the left subtree.
        \end{enumerate}
    \end{itemize}

    \item \textbf{Design an $O(k\log k)$ time algorithm to find the kth smallest number from a set 
    of n numbers arranged in the form of a binary min-heap (where $n \gg k$). Improve the time 
    complexity to $O(k\log \log k)$. Note that the trivial solution takes $O(k\log n)$ time (i.e., 
    by simply doing extract min k times).}
    \href{http://160592857366.free.fr/joe/ebooks/ShareData/An%20Optimal%20Algorithm%20for%20Selection%20in%20a%20Min-Heap.pdf}{\textbf{[Hint]}}

    To achieve an $O(k\log k)$ algorithm, we can use a two-heap strategy. We can achieve this time by observing that 
    the $k^{th}$ smallest element in a min-heap will be at or above the $k^{th}$ level in the heap, which means 
    that section of the heap is all we need to interact with. However, we can't perform any of the heap-related operations 
    in our original heap since it contains all $n$ elements, which would give us a $O(k\log n)$ runtime. This is why 
    we need the second heap, which we can keep at a size of at most $k$, allowing us to achieve $O(\log k)$ heap operations.
    
    We will first start with the original heap, and a second heap that initially only contains the root of the original 
    heap. Then, for $k - 1$ steps, we can do the following:

    \begin{enumerate}
        \item extract the min element from the second heap
        \item find the extracted element's children in the original heap (they'll be at indices $2i$ and $2i + 1$, where 
        $i$ was the index of the extracted element in the original heap) and insert them into the second heap
    \end{enumerate}

    By the end of the $k - 1$ steps, the min element in the second heap will be the $k^{th}$ smallest element in the original 
    heap. For the $k$ iterations, we incur $\log k$ work to perform heap insertion and removal operations in the 
    second heap (remember, we do not actually do any re-heapification on the original heap, as we just do read-only interactions 
    with children nodes via array indexing; all heapify operations are done in the second heap). This gives us a final 
    runtime of $O(k\log k)$.

    To reach an $O(k\log \log k)$ runtime, we can use the approach suggested in the paper, which is more nuanced but is 
    still somewhat of an extension of the previous algorithm, in that we are still using an auxiliary heap but are 
    interested in solving the problem in subtrees of the heap at a time.

    Before describing the algorithm, I've described some terminology from the paper that must be discussed for it to make sense:

    \begin{itemize}
        \item a \textbf{clan} is a grouping of the elements from the input heap. In this algorithm, all clans will have a 
        size of $\lfloor \log k \rfloor$. The first clan ($C_1$) would contain the first $\lfloor \log k \rfloor$ smallest 
        elements, the second clan $C_2$ would have the 2nd $\lfloor \log k \rfloor$ smallest, etc. Clans are generated 
        from previous clans, one by one in an iterative manner, as we progress through the algorithm.
        
        \item \textbf{offspring} of the $i^{th}$ clan, denoted $os(C_i)$ in the paper, are simply the $\lfloor \log k \rfloor$ smallest 
        children of the elements of a clan (from the original heap) that are not also in the clan. For example, if an arbitrary 
        node (that has 2 children) and its right child are both part of a clan, its offspring set could include the node's left 
        child and both of its right child's children, but not the right child itself.

        \begin{itemize}
            \item in the paper's example, a clan being ``grown from'' another clan simply means that the previous clan's 
            $\lfloor \log k \rfloor$ smallest offspring are selected to form the new clan. Keep in mind that a clan can have 
            more than $\lfloor \log k \rfloor$ offspring, but here only the smallest $\lfloor \log k \rfloor$ are selected
        \end{itemize}

        \item the \textbf{poor relations} of a clan $C_i$ are any offspring of the $C_{i - 1}$-st clan than were not part 
        of $C_i$. As mentioned in the previous point, a clan can have more than $\lfloor \log k \rfloor$ offspring; these 
        leftover offspring form the poor relations of the next clan.

        \item the \textbf{representative} of a clan is just the largest element in that clan
    \end{itemize}

    Without further ado, here's how the SEL1 algorithm would go about finding the $k^{th}$ smallest element in 
    $O(k\log \log k)$ time:
    
    \begin{enumerate}
        \item we start with the input heap $H_0$ and an auxiliary heap $H_2$
        \item use $H_2$ to find the $\lfloor \log k \rfloor$ smallest elements in $H_0$; these will form our first clan. 
        Determine the set of this clan's offspring $os(C_1)$, and set $pr(C_1)$ to the empty set (as there are no 
        poor relations for the first clan).
        \item find the representative of $C_1$ and insert it into a different heap, $H_1$
        \item For $\lceil \frac{k}{\lfloor \log k \rfloor} \rceil$ steps, do the following:
        \begin{enumerate}
            \item extract the min element of $H_1$ (initially the representative of $C_1$)
            \begin{itemize}
                \item let $C_j$ be the clan that the extracted element was a part of
                \item with $H_2$, find the $\lfloor \log k \rfloor$ smallest offspring of $C_j$; these elements will form 
                clan $C_i$ ($i$ represents the number of clans generated until now)
            \end{itemize}
            \item determine this new clan's offspring set and it's associated poor relations set
            \item if the poor relations set is non-empty, again find the $\lfloor \log k \rfloor$ smallest offspring, but 
            this time relative to the elements in the poor relations set. These will be clan $C_{i + 1}$
            \item again determine the offspring set and poor relations set for $C_{i + 1}$
            \item insert the representatives of both $C_i$ (the clan formed from the previous clan's offspring) and 
            $C_{i + 1}$ (the clan formed from the new clan's poor relations, i.e. the next $\lfloor \log k \rfloor$ elements 
            that were the previous clan's offspring but were not chosen to form $C_i$) into $H_1$
        \end{enumerate}
        \item using the last element removed from $H_1$ at the end of the loop, find the set of elements in $H_0$ that 
        are $\leq$ that last-removed element. Note that the last-removed element from $H_1$ will be the largest of the 
        $\lceil \frac{k}{\lfloor \log k \rfloor} \rceil$ clan representatives that we traversed through in the above loop, 
        making it at least as large as the $k^{th}$ largest element.
        \item from the set of elements achieved in step (e), directly select the $k^{th}$ smallest element, which can be done in 
        using a linear-time selection algorithm like median of medians or PICK (in this case, $O(k)$, since the set of elements selected will be approximately $k$)
    \end{enumerate}

    The runtime of this algorithm is $O(k\log\log k)$. The loop executes $\lceil \frac{k}{\lfloor \log k \rfloor} \rceil$ 
    steps, and each time we do $\log k$ work (for the heap operations in $H_1$), giving us $O(k)$. However, in each step, 
    we are also forming clans, which we'll have at most $2\lceil \frac{k}{\lfloor \log k \rfloor} \rceil + 1$ of, and 
    the time taken to create each one is $O(\log k \log\log k)$. Simplifying a bit by disregarding coefficients and 
    constants, we see that we'd thus take $\frac{k}{\log k} \cdot \log k \log\log k = O(k \log\log k)$ time to create 
    all the needed clans. Finally, after the loop ends, we end up with the set containing $O(k)$ elements that are less 
    than the element last removed from $H_1$, and selecting the $k^{th}$ smallest element from this set would take 
    linear time $O(k)$. So, we'd have a runtime of $O(k \log\log k + k) = O(k \log\log k)$.

    \item \textbf{Recall how we solved the following problem in class: Given a (read-only) 
    array A[1,n] of $n$ numbers, if it exists, find the number that appears more than n/2 
    times. Note that the time complexity for that approach is O(n). Also 
    note that A is ``read-only,'' and the auxiliary space used (i.e., to maintain two variables) 
    is O(1). Now, let's generalize this problem: Given a (read-only) array A of $n$ numbers and 
    an integer $k > 1$, determine the numbers that appear more than n/k times, if they exist. 
    Design an O(n log k) time solution that uses only O(k) auxiliary space.}

    We can solve this problem in a manner similar to the one discussed in class, just modified to 
    track more than one possible frequently-occurring element. The algorithm would go like this:

    \begin{enumerate}
        \item initialize an auxiliary data structure like an associative binary search tree (or hash map to give us 
        a better expected runtime but worse worst-case runtime) of size $k - 1$ to hold the elements that can 
        \textit{potentially} occur more than $\frac{n}{k}$ times. Note that if $k = 2$, we'd 
        arrive at the problem discussed in class and the array would be of size 1 (to hold the 
        one element appearing more than $\frac{n}{2}$ times).
        
        \item Like the algorithm discussed in class, walk through the array element by element, 
        but now we track $k - 1$ elements by mapping the key (the element of the array) to a ``count-like'' value. 
        The first time the element is seen, this value would be 1.
        
        \item Again similar to the base algorithm, we'd increment a key's count if we see it again, but 
        only if its key already exists in the array. If it doesn't (i.e. this is the first time we're 
        seeing the element), we'd make a new key in the array for it (also initialized to 1).
        
        \item In the case that the array is already filled and we can't make a new key for a new unique 
        element, we'd instead decrement the values of \textit{all} keys in the array by 1. And 
        as was the case in the base algorithm, if the count of a key ever reaches 0, we remove it (this is 
        analogous to what we did before, in that it will allow a new, potentially unique element into the 
        ``top-k-frequently-occurring'' group, and previously that group had a size of 1).

        \item Continue this process as described in steps 2 - 4 (inclusive) until the end of the 
        input array is reached. At this point, the array of $k - 1$ elements will contain all 
        the candidates for occurring more than $\frac{n}{k}$ times; however, it's not guaranteed 
        that \textit{every} element in that array will occur more than $\frac{n}{k}$ times (for example, 
        the last element in the array could've entered the array just once at the very end and 
        taken a slot thus only having a count of 1).

        \item Because of this, it's necessary to again scan the input array to confirm whether the final 
        $k - 1$ candidates actually occur more than $\frac{n}{k}$ times. We'd simply iterate over the input array 
        again and maintain counts of only the final candidates; any that occur less than or equal to $\frac{n}{k}$ 
        times can be safely discarded. The remaining candidates will be our final result.
    \end{enumerate}

    This approach runs in $O(n\log k)$ time with $O(k)$ space if we use a binary search tree/sorted array approach 
    for storing the size $k - 1$ candidate pool. This is because, for every element in the array, we need to 
    find whether its key exists by performing a binary search in the tree, which will take $O(\log k)$ time. If the 
    key doesn't exist, we'd have to create it in the tree (which also takes $O(\log k)$ time). Incrementing/decrementing 
    the count attribute of a key would be a constant-time operation. Thus, for all $n$ elements in the array, 
    we'd incur $O(\log k)$ work, giving us a runtime of $O(n\log k)$. The scan to confirm the validity of the final 
    $k - 1$ candidates would also take $O(n\log k)$ time since for all $n$ elements, we do a binary search to 
    determine whether it is a final candidate, and if it is, maintain a count for it.

    If we instead use a hash map, we'd get an expected runtime of $O(n)$ with $O(k)$ space, as each lookup or key 
    insertion into the size $k - 1$ hash map would be amortized $O(1)$. Of course, in the worst case, the runtime would 
    be $O(n \cdot k)$ due to the possibility of hash collisions.
\end{enumerate}

\end{document}
